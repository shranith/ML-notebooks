{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text-classification-20ng.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/shranith/ML-notebooks/blob/master/text_classification_20ng.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "feFbqZpP1soY",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Building a text classification model with TF Hub\n",
        "\n",
        "In this notebook, we'll walk you through building a model to predict the class of a document given its description into one of the 20 new group classes. The emphasis here is not on accuracy, but instead how to use TF Hub layers in a text classification model.\n",
        "\n",
        "\n",
        "The 20 Newsgroups data set is a collection of approximately 20,000 newsgroup documents, partitioned (nearly) evenly across 20 different newsgroups. To the best of my knowledge, it was originally collected by Ken Lang, probably for his Newsweeder: Learning to filter netnews paper, though he does not explicitly mention this collection. The 20 newsgroups collection has become a popular data set for experiments in text applications of machine learning techniques, such as text classification and text clustering.\n",
        "\n",
        "To start, import the necessary dependencies for this project."
      ]
    },
    {
      "metadata": {
        "id": "rOEllRxGQ_me",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ca8c7261-5f7c-4152-b8fd-6a7cf64d06cf"
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub\n",
        "import json\n",
        "import pickle\n",
        "import urllib\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.12.0-rc2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yLcRa_EjdehS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a86ec667-813b-46d2-a069-f20908f6fec5"
      },
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gw07kbavdnhX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "f3a06d3c-b1fe-4ee0-d72c-5d6a9aa586bd"
      },
      "cell_type": "code",
      "source": [
        "# Anyone can add the 20ng preprocessed dataset into csv format from google drive here https://drive.google.com/drive/folders/1xaQS8KsGWu7eQSZVcVYkNjFmmTaSXgpr?usp=sharing\n",
        "\n",
        "data = pd.read_csv('/content/gdrive/My Drive/20ng/20news-bydate-train.csv')\n",
        "print(data.head())\n",
        "\n",
        "descriptions = data['text']\n",
        "category = data['category']\n",
        "\n",
        "\n",
        "category[:10]\n"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   text_id                                               text  \\\n",
            "0     5701  From: nrmendel@unix.amherst.edu (Nathaniel Men...   \n",
            "1     1387  From: orly@phakt.usc.edu (Mr. Nitro Plastique)...   \n",
            "2     8953  From: hays@ssd.intel.com (Kirk Hays)\\n Subject...   \n",
            "3     2880  From: rnichols@cbnewsg.cb.att.com (robert.k.ni...   \n",
            "4     4348  From: steve-b@access.digex.com (Steve Brinich)...   \n",
            "\n",
            "                  category  \n",
            "0          rec.motorcycles  \n",
            "1    comp.sys.mac.hardware  \n",
            "2       talk.politics.guns  \n",
            "3  comp.os.ms-windows.misc  \n",
            "4                sci.crypt  \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0            rec.motorcycles\n",
              "1      comp.sys.mac.hardware\n",
              "2         talk.politics.guns\n",
              "3    comp.os.ms-windows.misc\n",
              "4                  sci.crypt\n",
              "5                alt.atheism\n",
              "6             comp.windows.x\n",
              "7                alt.atheism\n",
              "8                  sci.crypt\n",
              "9                  sci.crypt\n",
              "Name: category, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "metadata": {
        "id": "JmOD3yjV_lS3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "12a25736-2b6f-44f5-8af3-47299bba81dc"
      },
      "cell_type": "code",
      "source": [
        "type(descriptions)\n",
        "descriptions[:10]\n",
        "\n",
        "print(type(category[1]))\n",
        "type(category)\n"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'str'>\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pandas.core.series.Series"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "metadata": {
        "id": "ZUypuN818T_D",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Splitting our data\n",
        "When we train our model, we'll use 80% of the data for training and set aside 20% of the data to evaluate how our model performed."
      ]
    },
    {
      "metadata": {
        "id": "_nticMcj1alW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_size = int(len(descriptions) * .8)\n",
        "\n",
        "train_descriptions = descriptions[:train_size].astype('str')\n",
        "train_category = category[:train_size]\n",
        "\n",
        "test_descriptions = descriptions[train_size:].astype('str')\n",
        "test_category = category[train_size:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x5SLOv9JFbOy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print(test_category)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FmZ9iqK88nSD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Formatting our labels\n",
        "When we train our model we'll provide the labels (in this case genres) associated with each movie. We can't pass the genres in as strings directly, we'll transform them into multi-hot vectors. Since we have 9 genres, we'll have a 9 element vector for each movie with 0s and 1s indicating which genres are present in each description."
      ]
    },
    {
      "metadata": {
        "id": "bouv0R-D7J45",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "5b20fa73-e55a-4027-9998-b2ff335c4cd6"
      },
      "cell_type": "code",
      "source": [
        "encoder = LabelEncoder()\n",
        "encoder.fit_transform(train_category)\n",
        "train_encoded = encoder.transform(train_category)\n",
        "test_encoded = encoder.transform(test_category)\n",
        "num_classes = len(encoder.classes_)\n",
        "\n",
        "# Print all possible genres and the labels for the first movie in our training dataset\n",
        "print(encoder.classes_)\n",
        "print(train_encoded[0])"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['alt.atheism' 'comp.graphics' 'comp.os.ms-windows.misc'\n",
            " 'comp.sys.ibm.pc.hardware' 'comp.sys.mac.hardware' 'comp.windows.x'\n",
            " 'misc.forsale' 'rec.autos' 'rec.motorcycles' 'rec.sport.baseball'\n",
            " 'rec.sport.hockey' 'sci.crypt' 'sci.electronics' 'sci.med' 'sci.space'\n",
            " 'soc.religion.christian' 'talk.politics.guns' 'talk.politics.mideast'\n",
            " 'talk.politics.misc' 'talk.religion.misc']\n",
            "8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Ir8ez0K_9sYA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Create our TF Hub embedding layer\n",
        "[TF Hub]() provides a library of existing pre-trained model checkpoints for various kinds of models (images, text, and more) In this model we'll use the TF Hub `universal-sentence-encoder` module for our pre-trained word embeddings. We only need one line of code to instantiate module. When we train our model, it'll convert our array of movie description strings to embeddings. When we train our model, we'll use this as a feature column.\n"
      ]
    },
    {
      "metadata": {
        "id": "PWuNUXq7a-7p",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "description_embeddings = hub.text_embedding_column(\"descriptions\", module_spec=\"https://tfhub.dev/google/universal-sentence-encoder/3\", trainable=False)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9vscf4Fo-iI-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Instantiating our DNNEstimator Model\n",
        "The first parameter we pass to our DNNEstimator is called a head, and defines the type of labels our model should expect. Since we want our model to output one of the multiple labels, we’ll use multi_class_head here. Then we'll convert our features and labels to numpy arrays and instantiate our Estimator. `batch_size` and `num_epochs` are hyperparameters - you should experiment with different values to see what works best on your dataset."
      ]
    },
    {
      "metadata": {
        "id": "c0Vsmu9O21je",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "multi_label_head = tf.contrib.estimator.multi_class_head(\n",
        "    num_classes,\n",
        "    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8mTpWD_Q8GKe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "features = {\n",
        "  \"descriptions\": np.array(train_descriptions).astype(np.str)\n",
        "}\n",
        "labels = np.array(train_encoded).astype(np.int32)\n",
        "train_input_fn = tf.estimator.inputs.numpy_input_fn(features, labels, shuffle=True, batch_size=32, num_epochs=25)\n",
        "estimator = tf.contrib.estimator.DNNEstimator(\n",
        "    head=multi_label_head,\n",
        "    hidden_units=[64,10],\n",
        "    feature_columns=[description_embeddings])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5ak1cZPZ_ZYM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training and serving our model \n",
        "To train our model, we simply call `train()` passing it the input function we defined above. Once our model is trained, we'll define an evaluation input function similar to the one above and call `evaluate()`. When this completes we'll get a few metrics we can use to evaluate our model's accuracy.\n"
      ]
    },
    {
      "metadata": {
        "id": "jmtvJ5o3Olcg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "estimator.train(input_fn=train_input_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dMgti0YmJO7F",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Define our eval input_fn and run eval\n",
        "eval_input_fn = tf.estimator.inputs.numpy_input_fn({\"descriptions\": np.array(test_descriptions).astype(np.str)}, test_encoded.astype(np.int32), shuffle=False)\n",
        "estimator.evaluate(input_fn=eval_input_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mcPyCfmWABVO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Generating predictions on new data\n",
        "Now for the most fun part! Let's generate predictions on random descriptions our model hasn't seen before. We'll define an array of 3 new description strings (the comments indicate the correct genres) and create a `predict_input_fn`. Then we'll display the top 2 categories along with their confidence percentages for each of the 3 descriptions"
      ]
    },
    {
      "metadata": {
        "id": "ixlCKF6NEkTx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Test our model on some raw description data\n",
        "raw_test = [\n",
        "    \"The attacking midfielder came on as a substitute in the 1-0 defeat to Pep Guardiola's side having not played since September's Carabao Cup win against Watford because of a hamstring injury.\", # sports\n",
        "    \"On Twitter on Tuesday, West said he supports prison reform, common-sense gun laws and compassion for people seeking asylum, then denied that he had designed a logo for a branding exercise known as “Blexit,” which urges African Americans to leave the Democratic party. The concept, originated by Owens, claimed that West had designed the group’s merchandise.\", # Politics\n",
        "    \"From: ahmeda@McRCIM.McGill.EDU (Ahmed Abu-Abed)\\nSubject: Re: Desertification of the Negev\\nOriginator: ahmeda@ice.mcrcim.mcgill.edu\\nNntp-Posting-Host: ice.mcrcim.mcgill.edu\\nOrganization: McGill Research Centre for  Intelligent Machines\\nLines: 23\\n\\n\\nIn article <1993Apr26.021105.25642@cs.brown.edu>, dzk@cs.brown.edu (Danny Keren) writes:\\n|> This is nonsense. I lived in the Negev for many years and I can say\\n|> for sure that no Beduins were \\\"moved\\\" or harmed in any way. On the\\n|> contrary, their standard of living has climbed sharply; many of them\\n|> now live in rather nice, permanent houses, and own cars. There are\\n|> quite a few Beduin students in the Ben-Gurion university. There are\\n|> good, friendly relations between them and the rest of the population.\\n|> \\n|> All the Beduins I met would be rather surprised to read Mr. Davidson's\\n|> poster, I have to say.\\n|> \\n|> -Danny Keren.\\n|> \\n\\nIt is nonsense, Danny, if you can refute it with proof. If you are citing your\\nexperience then you should have been there in the 1940's (the article is\\ncomparing the condition then with that now).\\n\\nOtherwise, it is you who is trying to change the facts.\\n\\n-Ahmed.\\n\", # politics.middleeast\n",
        "]\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XHpMIWFsE4OB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Generate predictions\n",
        "predict_input_fn = tf.estimator.inputs.numpy_input_fn({\"descriptions\": np.array(raw_test).astype(np.str)}, shuffle=False)\n",
        "results = estimator.predict(predict_input_fn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iMVzrHpPDvoy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Display predictions\n",
        "for categories in results:\n",
        "  top_2 = categories['probabilities'].argsort()[-2:][::-1]\n",
        "  for category in top_2:\n",
        "    text_category = encoder.classes_[category]\n",
        "    print(text_category + ': ' + str(round(categories['probabilities'][category] * 100, 2)) + '%')\n",
        "  print('')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CfZTfK-e7MJr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}